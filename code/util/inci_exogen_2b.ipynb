{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Metrics\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Regresión con polinomio\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ubicación de los archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir('C:/Users/2BTOLIS-998/Documents/Proyectos/Macro/')\n",
    "os.chdir('C:/Users/jmoscoso04/OneDrive - Cementos Argos S.A/Documentos/Proyectos/2B/Macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del DF con todas las variables exogénas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "twobe = pd.read_excel('Ventas_Cantidad_Canal_Amz_2B.xlsx')\n",
    "twobe = twobe[['Fecha','cantidad']]\n",
    "twobe.columns = ['Fecha', 'value']\n",
    "twobe[\"año\"] = twobe[\"Fecha\"].dt.year\n",
    "twobe[\"mes\"] = twobe[\"Fecha\"].dt.month\n",
    "twobe[\"fecha\"] = twobe.apply(lambda row: str(row[\"año\"])+\"-\" \"0\"+str(row[\"mes\"]) \\\n",
    "                        if row[\"mes\"]<10 else str(row[\"año\"])+\"-\"+str(row[\"mes\"]),axis=1)\n",
    "twobe = pd.DataFrame(twobe.groupby('fecha')['value'].mean()).reset_index()\n",
    "twobe = twobe[['fecha','value']]\n",
    "twobe.columns = ['fecha', '2b']\n",
    "\n",
    "amazon = pd.read_excel('AMZN.xlsx')\n",
    "amazon = amazon[['Fecha','Close']]\n",
    "amazon.columns = ['Fecha', 'value']\n",
    "amazon[\"año\"] = amazon[\"Fecha\"].dt.year\n",
    "amazon[\"mes\"] = amazon[\"Fecha\"].dt.month\n",
    "amazon[\"fecha\"] = amazon.apply(lambda row: str(row[\"año\"])+\"-\" \"0\"+str(row[\"mes\"]) \\\n",
    "                        if row[\"mes\"]<10 else str(row[\"año\"])+\"-\"+str(row[\"mes\"]),axis=1)\n",
    "amazon = pd.DataFrame(amazon.groupby('fecha')['value'].mean()).reset_index()\n",
    "amazon = amazon[['fecha','value']]\n",
    "amazon.columns = ['fecha', 'amz']\n",
    "\n",
    "unemploy = pd.read_excel('Unemployment.xlsx')\n",
    "unemploy = unemploy[['Fecha','Actual']]\n",
    "unemploy.columns = ['Fecha', 'value']\n",
    "unemploy[\"año\"] = unemploy[\"Fecha\"].dt.year\n",
    "unemploy[\"mes\"] = unemploy[\"Fecha\"].dt.month\n",
    "unemploy[\"fecha\"] = unemploy.apply(lambda row: str(row[\"año\"])+\"-\" \"0\"+str(row[\"mes\"]) \\\n",
    "                        if row[\"mes\"]<10 else str(row[\"año\"])+\"-\"+str(row[\"mes\"]),axis=1)\n",
    "unemploy = pd.DataFrame(unemploy.groupby('fecha')['value'].mean()).reset_index()\n",
    "unemploy = unemploy[['fecha','value']]\n",
    "unemploy.columns = ['fecha', 'unempl']\n",
    "\n",
    "federal_funds = pd.read_excel('Datos históricos Federal Funds Composite Interest Rate.xlsx')\n",
    "federal_funds = federal_funds[['Fecha','Último']]\n",
    "federal_funds.columns = ['Fecha', 'value']\n",
    "federal_funds[\"value\"] = federal_funds[\"value\"]/100\n",
    "federal_funds[\"año\"] = federal_funds[\"Fecha\"].dt.year\n",
    "federal_funds[\"mes\"] = federal_funds[\"Fecha\"].dt.month\n",
    "federal_funds[\"fecha\"] = federal_funds.apply(lambda row: str(row[\"año\"])+\"-\" \"0\"+str(row[\"mes\"]) \\\n",
    "                        if row[\"mes\"]<10 else str(row[\"año\"])+\"-\"+str(row[\"mes\"]),axis=1)\n",
    "federal_funds = pd.DataFrame(federal_funds.groupby('fecha')['value'].mean()).reset_index()\n",
    "federal_funds = federal_funds[['fecha','value']]\n",
    "federal_funds.columns = ['fecha', 'federal_funds']\n",
    "\n",
    "apparel_retailers_dj_index = pd.read_excel('Dow Jones Apparel Retailers Historical Data.xlsx')\n",
    "apparel_retailers_dj_index = apparel_retailers_dj_index[['Fecha','Price']]\n",
    "apparel_retailers_dj_index.columns = ['Fecha', 'value']\n",
    "apparel_retailers_dj_index.value = apparel_retailers_dj_index.value.apply(int)\n",
    "apparel_retailers_dj_index[\"año\"] = apparel_retailers_dj_index[\"Fecha\"].dt.year\n",
    "apparel_retailers_dj_index[\"mes\"] = apparel_retailers_dj_index[\"Fecha\"].dt.month\n",
    "apparel_retailers_dj_index[\"fecha\"] = apparel_retailers_dj_index.apply(lambda row: str(row[\"año\"])+\"-\" \"0\"+str(row[\"mes\"]) \\\n",
    "                        if row[\"mes\"]<10 else str(row[\"año\"])+\"-\"+str(row[\"mes\"]),axis=1)\n",
    "apparel_retailers_dj_index = pd.DataFrame(apparel_retailers_dj_index.groupby('fecha')['value'].mean()).reset_index()\n",
    "apparel_retailers_dj_index = apparel_retailers_dj_index[['fecha','value']]\n",
    "apparel_retailers_dj_index.columns = ['fecha', 'apparel_retailers']\n",
    "\n",
    "tes_10y = pd.read_excel('Estados Unidos 10 años Datos Históricos Rendimiento de Bonos.xlsx')\n",
    "tes_10y = tes_10y[['Fecha','Último']]\n",
    "tes_10y.columns = ['Fecha', 'value']\n",
    "tes_10y[\"value\"] = tes_10y[\"value\"]/100\n",
    "tes_10y[\"año\"] = tes_10y[\"Fecha\"].dt.year\n",
    "tes_10y[\"mes\"] = tes_10y[\"Fecha\"].dt.month\n",
    "tes_10y[\"fecha\"] = tes_10y.apply(lambda row: str(row[\"año\"])+\"-\" \"0\"+str(row[\"mes\"]) \\\n",
    "                        if row[\"mes\"]<10 else str(row[\"año\"])+\"-\"+str(row[\"mes\"]),axis=1)\n",
    "tes_10y = pd.DataFrame(tes_10y.groupby('fecha')['value'].mean()).reset_index()\n",
    "tes_10y = tes_10y[['fecha','value']]\n",
    "tes_10y.columns = ['fecha', 'test_10y']\n",
    "\n",
    "gdp = pd.read_excel('GDP.xlsx')\n",
    "gdp = gdp[['Fecha','Actual']]\n",
    "gdp.columns = ['Fecha', 'value']\n",
    "gdp[\"año\"] = gdp[\"Fecha\"].dt.year\n",
    "gdp[\"mes\"] = gdp[\"Fecha\"].dt.month\n",
    "gdp[\"fecha\"] = gdp.apply(lambda row: str(row[\"año\"])+\"-\" \"0\"+str(row[\"mes\"]) \\\n",
    "                        if row[\"mes\"]<10 else str(row[\"año\"])+\"-\"+str(row[\"mes\"]),axis=1)\n",
    "gdp = gdp[['fecha','value']]\n",
    "gdp.columns = ['fecha', 'gdp']\n",
    "\n",
    "inflation = pd.read_excel('Inflación US.xlsx')\n",
    "inflation = inflation[['Fecha','Actual']]\n",
    "inflation.columns = ['Fecha', 'value']\n",
    "inflation[\"año\"] = inflation[\"Fecha\"].dt.year\n",
    "inflation[\"mes\"] = inflation[\"Fecha\"].dt.month\n",
    "inflation[\"fecha\"] = inflation.apply(lambda row: str(row[\"año\"])+\"-\" \"0\"+str(row[\"mes\"]) \\\n",
    "                        if row[\"mes\"]<10 else str(row[\"año\"])+\"-\"+str(row[\"mes\"]),axis=1)\n",
    "inflation = inflation[['fecha','value']]\n",
    "inflation.columns = ['fecha', 'inflation']\n",
    "\n",
    "consumer_discretionary_sp_index = pd.read_excel('S&P 500 Consumer Discretionary index.xlsx')\n",
    "consumer_discretionary_sp_index = consumer_discretionary_sp_index[['Fecha','S&P 500 Consumer Discretionary (Sector)']]\n",
    "consumer_discretionary_sp_index.columns = ['Fecha', 'value']\n",
    "consumer_discretionary_sp_index[\"año\"] = consumer_discretionary_sp_index[\"Fecha\"].dt.year\n",
    "consumer_discretionary_sp_index[\"mes\"] = consumer_discretionary_sp_index[\"Fecha\"].dt.month\n",
    "consumer_discretionary_sp_index[\"fecha\"] = consumer_discretionary_sp_index.apply(lambda row: str(row[\"año\"])+\"-\" \"0\"+str(row[\"mes\"]) \\\n",
    "                        if row[\"mes\"]<10 else str(row[\"año\"])+\"-\"+str(row[\"mes\"]),axis=1)\n",
    "consumer_discretionary_sp_index = pd.DataFrame(consumer_discretionary_sp_index.groupby('fecha')['value'].mean()).reset_index()\n",
    "consumer_discretionary_sp_index = consumer_discretionary_sp_index[['fecha','value']]\n",
    "consumer_discretionary_sp_index.columns = ['fecha', 'consumer_discretionary']\n",
    "\n",
    "luxury_sp_index = pd.read_excel('S&P Global Luxury Index.xlsx')\n",
    "luxury_sp_index = luxury_sp_index[['Fecha','S&P Global Luxury Index']]\n",
    "luxury_sp_index.columns = ['Fecha', 'value']\n",
    "luxury_sp_index[\"año\"] = luxury_sp_index[\"Fecha\"].dt.year\n",
    "luxury_sp_index[\"mes\"] = luxury_sp_index[\"Fecha\"].dt.month\n",
    "luxury_sp_index[\"fecha\"] = luxury_sp_index.apply(lambda row: str(row[\"año\"])+\"-\" \"0\"+str(row[\"mes\"]) \\\n",
    "                        if row[\"mes\"]<10 else str(row[\"año\"])+\"-\"+str(row[\"mes\"]),axis=1)\n",
    "luxury_sp_index = pd.DataFrame(luxury_sp_index.groupby('fecha')['value'].mean()).reset_index()\n",
    "luxury_sp_index = luxury_sp_index[['fecha','value']]\n",
    "luxury_sp_index.columns = ['fecha', 'luxury']\n",
    "\n",
    "decisiones_fed = pd.read_excel('Decisiones de PM FED.xlsx')\n",
    "decisiones_fed = decisiones_fed[['Fecha','Actual']]\n",
    "decisiones_fed.columns = ['Fecha', 'value']\n",
    "decisiones_fed[\"año\"] = decisiones_fed[\"Fecha\"].dt.year\n",
    "decisiones_fed[\"mes\"] = decisiones_fed[\"Fecha\"].dt.month\n",
    "decisiones_fed[\"fecha\"] = decisiones_fed.apply(lambda row: str(row[\"año\"])+\"-\" \"0\"+str(row[\"mes\"]) \\\n",
    "                        if row[\"mes\"]<10 else str(row[\"año\"])+\"-\"+str(row[\"mes\"]),axis=1)\n",
    "decisiones_fed = pd.DataFrame(decisiones_fed.groupby('fecha')['value'].mean()).reset_index()\n",
    "decisiones_fed = decisiones_fed[['fecha','value']]\n",
    "decisiones_fed.columns = ['fecha', 'fed_mp']\n",
    "\n",
    "list_df = [twobe, amazon, unemploy, federal_funds, apparel_retailers_dj_index, tes_10y,\n",
    "            gdp,inflation,consumer_discretionary_sp_index,luxury_sp_index,decisiones_fed]\n",
    "\n",
    "df = list_df[0]\n",
    "for df_ in list_df[1:]:\n",
    "    df = df.merge(df_, on='fecha', how = 'left')\n",
    "df = df[(df.fecha >= '2015-01') & (df.fecha <= '2022-06')]\n",
    "df = df.set_index('fecha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap matriz de correlaciones\n",
    "# ==============================================================================\n",
    "# features = ['2b','amz', 'apparel_retailers', 'consumer_discretionary', 'luxury']\n",
    "# features = ['federal_funds', 'test_10y', 'gdp','inflation', 'fed_mp']\n",
    "features = ['2b', 'federal_funds', 'test_10y', 'gdp','inflation', 'unempl','fed_mp']\n",
    "df = df[features]\n",
    "\n",
    "df_d = (df / df.shift(1)) -1\n",
    "df_d = df_d.replace([np.inf],0) \n",
    "df_d = df_d.replace([-np.inf],0)\n",
    "# corr_matrix = df_d[1:].corr()\n",
    "# mask_ut = np.triu(np.ones(corr_matrix.shape)).astype(np.bool)\n",
    "\n",
    "# fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5))\n",
    "# sns.heatmap(\n",
    "#     corr_matrix,\n",
    "#     mask      = mask_ut,\n",
    "#     annot     = True,\n",
    "#     cbar      = False,\n",
    "#     annot_kws = {\"size\": 7},\n",
    "#     vmin      = -1,\n",
    "#     vmax      = 1,\n",
    "#     center    = 0,\n",
    "#     cmap      = sns.diverging_palette(20, 220, n=200),\n",
    "#     square    = True,\n",
    "#     ax        = ax\n",
    "    \n",
    "# )\n",
    "# ax.set_title('Coef. Corr. con Vbles. Macro')\n",
    "# ax.set_xticklabels(\n",
    "#     ax.get_xticklabels(),\n",
    "#     rotation = 45,\n",
    "#     horizontalalignment = 'right',\n",
    "# )\n",
    "# plt.savefig('corr.png')\n",
    "\n",
    "# ax.tick_params(labelsize = 8)  \n",
    "df_d = df_d[1:]\n",
    "# df_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node</th>\n",
       "      <th>feature</th>\n",
       "      <th>corr_best</th>\n",
       "      <th>lags_best</th>\n",
       "      <th>corr_OG</th>\n",
       "      <th>lags_OG</th>\n",
       "      <th>best</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2b</td>\n",
       "      <td>federal_funds</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>corr_best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2b</td>\n",
       "      <td>test_10y</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>corr_best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2b</td>\n",
       "      <td>gdp</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>-2</td>\n",
       "      <td>3</td>\n",
       "      <td>corr_best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2b</td>\n",
       "      <td>inflation</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>corr_best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2b</td>\n",
       "      <td>unempl</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>-16</td>\n",
       "      <td>3</td>\n",
       "      <td>corr_best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2b</td>\n",
       "      <td>fed_mp</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>corr_best</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  node        feature  corr_best  lags_best  corr_OG  lags_OG       best\n",
       "0   2b  federal_funds         16          8        3        3  corr_best\n",
       "1   2b       test_10y         30          0       30        3  corr_best\n",
       "2   2b            gdp         30         11       -2        3  corr_best\n",
       "3   2b      inflation         14          7        4        3  corr_best\n",
       "4   2b         unempl         25         11      -16        3  corr_best\n",
       "5   2b         fed_mp         10          0       10        3  corr_best"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_features = ['federal_funds', 'test_10y', 'gdp','inflation', 'unempl', 'fed_mp']\n",
    "\n",
    "pruebas = pd.DataFrame()\n",
    "for f in list_features:\n",
    "    pruebas_temp = pd.DataFrame()\n",
    "    justit = list(sm.tsa.stattools.ccf(df_d['2b'], df_d[f], adjusted=False))[0:12]\n",
    "    hoctus = justit.index(max(justit))\n",
    "    pruebas_temp['node'] = ['2b']\n",
    "    pruebas_temp['feature'] = [f]\n",
    "    pruebas_temp['lags'] = [hoctus]\n",
    "    pruebas_temp['value'] = [max(justit)]\n",
    "    pruebas = pd.concat([pruebas, pruebas_temp], axis = 0)\n",
    "\n",
    "correlations = pd.DataFrame()\n",
    "for f in list_features:\n",
    "    prue_lags = pd.DataFrame()\n",
    "    n_lags = pruebas[(pruebas.feature == f)].lags[0]\n",
    "    toro = pd.concat([df_d[['2b']], df_d[[f]].shift(n_lags)], axis = 1)\n",
    "    toro = toro[~toro['2b'].isnull()]\n",
    "    cor_ind = toro.corr().iloc[0][1]\n",
    "    prue_lags['node'] = ['2b']\n",
    "    prue_lags['feature'] = [f]\n",
    "    prue_lags['corr_best'] = [int(round(cor_ind*100,0))]\n",
    "    prue_lags['lags_best'] = [n_lags]\n",
    "    correlations = pd.concat([correlations, prue_lags], axis = 0)\n",
    "correlations.sort_values(by = ['corr_best'], ascending = False)\n",
    "correlations\n",
    "\n",
    "corr_matrix = df_d.corr()\n",
    "corr_matrix.reset_index()\n",
    "betha = pd.DataFrame()\n",
    "for f in list_features:\n",
    "    alpha = corr_matrix.reset_index()\n",
    "    alpha = alpha[['index', f]]\n",
    "    alpha['feature'] = f\n",
    "    alpha.columns = ['node', 'corr_OG','feature']\n",
    "    alpha = alpha[['node', 'feature','corr_OG']]\n",
    "    alpha['corr_OG'] = round(alpha['corr_OG']*100,0)\n",
    "    alpha['corr_OG'] = alpha['corr_OG'].apply(int)\n",
    "    betha = pd.concat([betha,alpha], axis = 0 )\n",
    "    \n",
    "bd_corr = correlations.merge(betha, on = ['node', 'feature'], how = 'inner')\n",
    "bd_corr['lags_OG'] = 3\n",
    "bd_corr['best'] = np.where(abs(bd_corr.corr_best) >= abs(bd_corr.corr_OG), 'corr_best', 'corr_OG')\n",
    "bd_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del DF con los lags correspondientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sin lags\n",
    "# ======================================================================\n",
    "# vela = df_d.copy()\n",
    "# X = vela.drop(columns = '2b').values\n",
    "# y = vela['2b'].values\n",
    "\n",
    "# x = scale(X)\n",
    "# y = scale(y)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5,random_state=123)\n",
    "\n",
    "# Con lags\n",
    "# ======================================================================\n",
    "vaca = pd.DataFrame(df_d[['2b']])\n",
    "\n",
    "for f in list_features:\n",
    "    n_lags = bd_corr[(bd_corr.feature == f)].lags_best.iloc[0]\n",
    "    vaca = pd.concat([vaca, df_d[[f]].shift(n_lags)], axis = 1)\n",
    "vaca  = vaca[max(correlations.lags_best):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de sugerencias de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vaca.drop(columns = '2b').values\n",
    "# X = sm.add_constant(X, prepend = True)\n",
    "y = vaca['2b'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5,random_state=123)\n",
    "\n",
    "reg = LazyRegressor(verbose=0,ignore_warnings=False,custom_metric=None)\n",
    "models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "El error (rmse) de ols es: 0.3262994990459682\n",
      "El r2 de ols es: -3.584308584399693\n"
     ]
    }
   ],
   "source": [
    "# X = vaca.drop(columns = '2b')\n",
    "# X = X.drop(columns = ['unempl', 'fed_mp', 'federal_funds'])\n",
    "# X = sm.add_constant(X, prepend = True)\n",
    "\n",
    "# y = vaca['2b']\n",
    "# ols_model = sm.OLS(endog=y, exog=X)\n",
    "# ols_model = ols_model.fit()\n",
    "# ols_model.summary()\n",
    "\n",
    "# ==============================================================================\n",
    "X = vaca.drop(columns = '2b')\n",
    "X = sm.add_constant(X, prepend = True)\n",
    "y = vaca['2b'].values\n",
    "\n",
    "# x = scale(X)\n",
    "# y = scale(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5,random_state=123)\n",
    "\n",
    "\n",
    "modelo = LinearRegression(normalize=True)\n",
    "modelo.fit(X = X_train, y = y_train)\n",
    "\n",
    "# Predicciones test\n",
    "# ==============================================================================\n",
    "predicciones = modelo.predict(X=X_test)\n",
    "predicciones = predicciones.flatten()\n",
    "predicciones[:10]\n",
    "\n",
    "# Error de test del modelo \n",
    "# ==============================================================================\n",
    "rmse_ols = mean_squared_error(\n",
    "            y_true  = y_test,\n",
    "            y_pred  = predicciones,\n",
    "            squared = False\n",
    "           )\n",
    "r2_ols = r2_score(\n",
    "            y_true  = y_test,\n",
    "            y_pred  = predicciones,\n",
    "           )\n",
    "print(\"\")\n",
    "print(f\"El error (rmse) de ols es: {rmse_ols}\")\n",
    "print(f\"El r2 de ols es: {(r2_ols)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDGRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.0626630232099864\n",
      "RMSE:  1.1516449780764226\n",
      "[ 0.2004777   0.15864918  0.03627261  0.43069708 -0.08490822 -0.09836266]\n",
      "[0.17698817]\n",
      "R2:  -0.10676199607803483\n"
     ]
    }
   ],
   "source": [
    "X = vaca.drop(columns = '2b').values\n",
    "y = vaca['2b'].values\n",
    "\n",
    "x = scale(X)\n",
    "y = scale(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5,random_state=123)\n",
    "\n",
    "sgdr = SGDRegressor(alpha=0.0001, epsilon=0.01, eta0=0.1,penalty='elasticnet')\n",
    "sgdr.fit(X_train, y_train)\n",
    "\n",
    "score = sgdr.score(X_train, y_train)\n",
    "print(\"R-squared:\", score)\n",
    "\n",
    "ypred = sgdr.predict(X_test)\n",
    "\n",
    "mse_sgd = mean_squared_error(y_test, ypred)\n",
    "r2_sgd = r2_score(\n",
    "            y_true  = y_test,\n",
    "            y_pred  = predicciones,\n",
    "           )\n",
    "print(\"RMSE: \", mse_sgd**(1/2.0))\n",
    "print(sgdr.coef_)\n",
    "print(sgdr.intercept_)\n",
    "print(\"R2: \", r2_sgd)\n",
    "# x_ax = range(len(y_test))\n",
    "# plt.plot(x_ax, y_test, label=\"original\")\n",
    "# plt.plot(x_ax, ypred, label=\"predicted\")\n",
    "# plt.title(\"2b quantity of sales test and predicted data\")\n",
    "# plt.xlabel('X-axis')\n",
    "# plt.ylabel('Y-axis')\n",
    "# plt.legend(loc='best',fancybox=True, shadow=True)\n",
    "# plt.grid(True)\n",
    "# plt.savefig('pron.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       predictor  coef\n",
      "0  federal_funds  0.00\n",
      "1       test_10y  0.00\n",
      "2            gdp  0.00\n",
      "3      inflation  0.00\n",
      "4         unempl -0.00\n",
      "5         fed_mp -0.00\n",
      "\n",
      "El error (rmse) de lasso es: 0.16032697708707516\n",
      "El r2 de lasso es: -0.1067619960780346\n"
     ]
    }
   ],
   "source": [
    "# Preprocesado y modelado\n",
    "# ==============================================================================\n",
    "# Con lags\n",
    "# ======================================================================\n",
    "X = vaca.drop(columns = '2b')\n",
    "y = vaca['2b']\n",
    "\n",
    "x = scale(X)\n",
    "y = scale(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5,random_state=123)\n",
    "\n",
    "# Sin lags\n",
    "# # ======================================================================\n",
    "# vela = df_d.copy()\n",
    "# X = vela.drop(columns = '2b').values\n",
    "# y = vela['2b'].values\n",
    "\n",
    "# x = scale(X)\n",
    "# y = scale(y)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5,random_state=123)\n",
    "# ======================================================================\n",
    "\n",
    "# Mejor modelo alpha óptimo + 1sd\n",
    "# ==============================================================================\n",
    "modelo = Lasso(alpha=0.05, normalize=True)\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Coeficientes del modelo\n",
    "# ==============================================================================\n",
    "df_coeficientes = pd.DataFrame(\n",
    "                        {'predictor': features[1:],\n",
    "                         'coef': modelo.coef_.flatten()}\n",
    "                  )\n",
    "\n",
    "# Predictores incluidos en el modelo (coeficiente != 0)\n",
    "df_coeficientes[df_coeficientes.coef != 0]\n",
    "print(df_coeficientes)\n",
    "\n",
    "# Predicciones test\n",
    "# ==============================================================================\n",
    "predicciones = modelo.predict(X=X_test)\n",
    "predicciones = predicciones.flatten()\n",
    "predicciones[:10]\n",
    "\n",
    "# Error de test del modelo \n",
    "# ==============================================================================\n",
    "rmse_lasso = mean_squared_error(\n",
    "                y_true  = y_test,\n",
    "                y_pred  = predicciones,\n",
    "                squared = False\n",
    "             )\n",
    "\n",
    "r2_lasso = r2_score(\n",
    "            y_true  = y_test,\n",
    "            y_pred  = predicciones,\n",
    "           )\n",
    "print(\"\")\n",
    "print(f\"El error (rmse) de lasso es: {rmse_lasso}\")\n",
    "print(f\"El r2 de lasso es: {(r2_lasso)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Lineal con Polinomio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefs</th>\n",
       "      <th>degree</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.025840531515534337, -1.6549231442247834, 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-17909345278897.81, -0.08823703396769011, -0....</td>\n",
       "      <td>2</td>\n",
       "      <td>46.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.7209244664992969, 17.096749322318104, -18....</td>\n",
       "      <td>3</td>\n",
       "      <td>4340.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.4734982162095574, 2.066502062351276, -11.7...</td>\n",
       "      <td>4</td>\n",
       "      <td>9863.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.33226689561172934, 6.036843086382415, -9.9...</td>\n",
       "      <td>5</td>\n",
       "      <td>51782.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.22912390045565678, 5.954894129884662, -11....</td>\n",
       "      <td>6</td>\n",
       "      <td>100610.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.21347635612346488, 6.085464994688557, -11....</td>\n",
       "      <td>7</td>\n",
       "      <td>370554.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.21249952190966176, 6.0940140751995155, -11...</td>\n",
       "      <td>8</td>\n",
       "      <td>1430423.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.20974328516018542, 6.08457036883677, -11.3...</td>\n",
       "      <td>9</td>\n",
       "      <td>5569482.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.2100426622135424, 6.057473623014131, -11.3...</td>\n",
       "      <td>10</td>\n",
       "      <td>12512456.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               coefs  degree        rmse\n",
       "0  [-0.025840531515534337, -1.6549231442247834, 1...       1        1.37\n",
       "0  [-17909345278897.81, -0.08823703396769011, -0....       2       46.35\n",
       "0  [-0.7209244664992969, 17.096749322318104, -18....       3     4340.95\n",
       "0  [-0.4734982162095574, 2.066502062351276, -11.7...       4     9863.20\n",
       "0  [-0.33226689561172934, 6.036843086382415, -9.9...       5    51782.81\n",
       "0  [-0.22912390045565678, 5.954894129884662, -11....       6   100610.28\n",
       "0  [-0.21347635612346488, 6.085464994688557, -11....       7   370554.45\n",
       "0  [-0.21249952190966176, 6.0940140751995155, -11...       8  1430423.94\n",
       "0  [-0.20974328516018542, 6.08457036883677, -11.3...       9  5569482.85\n",
       "0  [-0.2100426622135424, 6.057473623014131, -11.3...      10 12512456.30"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocesado y modelado\n",
    "# ==============================================================================\n",
    "# Con lags\n",
    "# ======================================================================\n",
    "# X = vaca.drop(columns = '2b')\n",
    "# y = vaca['2b']\n",
    "\n",
    "# x = scale(X)\n",
    "# y = scale(y)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5,random_state=123)\n",
    "\n",
    "# Sin lags\n",
    "# ======================================================================\n",
    "vela = df_d.copy()\n",
    "X = vela.drop(columns = '2b').values\n",
    "y = vela['2b'].values\n",
    "\n",
    "x = scale(X)\n",
    "y = scale(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5,random_state=123)\n",
    "# ======================================================================\n",
    "\n",
    "polinomios = pd.DataFrame()\n",
    "# Se evalúan los grados del polinomio\n",
    "for num_pol in range(1,11):\n",
    "    poli_reg = PolynomialFeatures(degree= num_pol)\n",
    "\n",
    "    # se transforma las caracteristicas existentes en caracteres de mayor grado\n",
    "    X_train_poli = poli_reg.fit_transform(X_train)\n",
    "    X_test_poli = poli_reg.fit_transform(X_test)\n",
    "\n",
    "    # Defino el algoritmo a utilizar y lo entreno\n",
    "    pr = linear_model.LinearRegression()\n",
    "    pr.fit(X_train_poli, y_train)\n",
    "\n",
    "    # predicción\n",
    "    y_pred_pr = pr.predict(X_test_poli)\n",
    "    rmse = mean_squared_error(y_test, y_pred_pr)**(1/2)\n",
    "\n",
    "    # Datos del modelo\n",
    "    coef = list(pr.coef_)\n",
    "    coef[0] = pr.intercept_\n",
    "    coef_temp = pd.DataFrame({'coefs':[coef]})\n",
    "    coef_temp['degree'] = [num_pol]\n",
    "    coef_temp['rmse'] = [rmse]\n",
    "    polinomios = pd.concat([polinomios, coef_temp])\n",
    "polinomios.sort_values(by='rmse', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearGAM                                                                                                 \n",
      "=============================================== ==========================================================\n",
      "Distribution:                        NormalDist Effective DoF:                                      3.3444\n",
      "Link Function:                     IdentityLink Log Likelihood:                                   -40.8455\n",
      "Number of Samples:                           33 AIC:                                               90.3799\n",
      "                                                AICc:                                               92.059\n",
      "                                                GCV:                                                0.9321\n",
      "                                                Scale:                                              0.7638\n",
      "                                                Pseudo R-Squared:                                   0.0828\n",
      "==========================================================================================================\n",
      "Feature Function                  Lambda               Rank         EDoF         P > x        Sig. Code   \n",
      "================================= ==================== ============ ============ ============ ============\n",
      "s(0)                              [0.6]                20                        1.11e-16     ***         \n",
      "s(1)                              [0.6]                20                        1.11e-16     ***         \n",
      "s(2)                              [0.6]                20                        1.11e-16     ***         \n",
      "s(3)                              [0.6]                20                        1.11e-16     ***         \n",
      "s(4)                              [0.6]                20                        1.11e-16     ***         \n",
      "s(5)                              [0.6]                20                        1.11e-16     ***         \n",
      "intercept                                              1                         2.84e-01                 \n",
      "==========================================================================================================\n",
      "Significance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n",
      "WARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n",
      "         which can cause p-values to appear significant when they are not.\n",
      "\n",
      "WARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n",
      "         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n",
      "         are typically lower than they should be, meaning that the tests reject the null too readily.\n",
      "RMSE:  1.1856293180187063\n",
      "[-0.00335464 -0.01291765 -0.00929191  0.00472779  0.00174063 -0.02405286\n",
      "  0.01209896  0.00504968 -0.02744669  0.0385541  -0.05262236  0.0065497\n",
      "  0.13348165  0.06810599  0.01178847 -0.03102631 -0.04374781  0.04677952\n",
      " -0.05609449  0.00943805  0.00158592 -0.01375619  0.01223804 -0.01386281\n",
      "  0.01155654  0.00605102 -0.01240526  0.0276509  -0.03043182  0.02035648\n",
      "  0.08028443  0.10774858  0.02209264 -0.06441316  0.00658959 -0.04750563\n",
      " -0.02794482  0.04042715 -0.05924563  0.01074386  0.0054124  -0.04047529\n",
      "  0.02444139 -0.01009025 -0.01195915  0.0261263  -0.02411595  0.00579049\n",
      "  0.01901092 -0.03513341  0.0300333  -0.00044363 -0.04170788  0.09345041\n",
      " -0.03583248  0.0281564   0.01931507 -0.01951129  0.04685847 -0.00156601\n",
      "  0.00379735 -0.02608653  0.00750505 -0.01788063 -0.0130839   0.02626004\n",
      " -0.01860638 -0.00426734  0.03819739 -0.04848408 -0.00729797  0.10290486\n",
      "  0.02590029 -0.01538173  0.04386587  0.02156901 -0.03875048  0.04108079\n",
      " -0.05154202  0.00806021 -0.00374837  0.0131146  -0.006349    0.04145988\n",
      "  0.14549262 -0.0684601  -0.05680893  0.05294756 -0.02407893  0.00274751\n",
      "  0.01047828 -0.01079684  0.00300738  0.00456144 -0.00581651  0.00068563\n",
      "  0.0045236  -0.00698701 -0.01540148 -0.0028115  -0.00480541  0.06400363\n",
      " -0.004566   -0.04966057  0.01372507  0.05236514  0.04505442 -0.04440132\n",
      "  0.06939299 -0.02891278  0.00565269  0.01118932 -0.01311604  0.00414424\n",
      "  0.00688188 -0.01113636  0.00306652  0.00737738 -0.04948264  0.00098765\n",
      "  0.07775981]\n",
      "R2:  1.4057168797455026\n"
     ]
    }
   ],
   "source": [
    "import warnings;\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Preprocesado y modelado\n",
    "# ==============================================================================\n",
    "# Con lags\n",
    "# ======================================================================\n",
    "X = vaca.drop(columns = '2b')\n",
    "y = vaca['2b']\n",
    "\n",
    "x = scale(X)\n",
    "y = scale(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5,random_state=123)\n",
    "\n",
    "# # Sin lags\n",
    "# # ======================================================================\n",
    "# vela = df_d.copy()\n",
    "# X = vela.drop(columns = '2b').values\n",
    "# y = vela['2b'].values\n",
    "\n",
    "# x = scale(X)\n",
    "# y = scale(y)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5,random_state=123)\n",
    "# ======================================================================\n",
    "\n",
    "from pygam import GAM, LinearGAM, s, f, te\n",
    "\n",
    "n_features = 7 # number of features used in the model\n",
    "lams = np.logspace(-5,5,20) * n_features\n",
    "splines = 12 # number of splines we will use\n",
    "# linear GAM for Regression\n",
    "gam = LinearGAM().fit(X_train, y_train)\n",
    "gam.summary()\n",
    "\n",
    "ypred = gam.predict(X_test)\n",
    "\n",
    "mse_gam = mean_squared_error(y_test, ypred)\n",
    "r2_gam = mean_squared_error(y_test, ypred)\n",
    "print(\"RMSE: \", mse_gam**(1/2.0))\n",
    "print(gam.coef_)\n",
    "print(\"R2: \", r2_gam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1de81215df8145a9b5b94dafaec65ad3161698da85c754c939603adfd603c3e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
